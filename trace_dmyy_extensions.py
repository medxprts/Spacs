#!/usr/bin/env python3
"""
Trace DMYY extension history from Dec 2023 to present
Find all extensions and current deadline
"""

import sys
import os
import requests
from bs4 import BeautifulSoup
from datetime import datetime
import json

sys.path.append('/home/ubuntu/spac-research')

from openai import OpenAI

# AI client for extraction
AI_CLIENT = OpenAI(
    api_key=os.getenv("DEEPSEEK_API_KEY"),
    base_url="https://api.deepseek.com"
)

HEADERS = {'User-Agent': 'LEVP SPAC Platform fenil@legacyevp.com'}


def fetch_recent_8ks():
    """Fetch all 8-K filings since Dec 2023"""
    print("üì• Fetching DMYY 8-K filings since Dec 2023...")

    url = "https://www.sec.gov/cgi-bin/browse-edgar?action=getcompany&CIK=0001915380&type=8-K&dateb=&owner=exclude&count=100&output=atom"

    response = requests.get(url, headers=HEADERS, timeout=30)
    response.raise_for_status()

    from xml.etree import ElementTree as ET
    root = ET.fromstring(response.content)

    # Parse atom feed
    ns = {'atom': 'http://www.w3.org/2005/Atom'}
    entries = root.findall('atom:entry', ns)

    filings = []
    for entry in entries:
        title_elem = entry.find('atom:title', ns)
        updated_elem = entry.find('atom:updated', ns)
        link_elem = entry.find('atom:link[@rel="alternate"]', ns)

        if title_elem is not None and updated_elem is not None and link_elem is not None:
            filing_date = datetime.fromisoformat(updated_elem.text.replace('Z', '+00:00'))

            # Only get filings from Dec 2023 onwards
            if filing_date >= datetime(2023, 12, 1, tzinfo=filing_date.tzinfo):
                filings.append({
                    'date': filing_date,
                    'url': link_elem.get('href'),
                    'title': title_elem.text
                })

    print(f"   Found {len(filings)} 8-K filings since Dec 2023\n")
    return filings


def fetch_filing_content(url: str) -> str:
    """Fetch filing HTML content"""

    # If it's an index page, parse it to find the primary document
    if '-index.htm' in url:
        # Fetch the index page
        response = requests.get(url, headers=HEADERS, timeout=30)
        response.raise_for_status()

        soup = BeautifulSoup(response.content, 'html.parser')

        # Find the table with filing documents
        # Look for the first .htm file (primary document) in the table
        for row in soup.find_all('tr'):
            cells = row.find_all('td')
            if len(cells) >= 3:
                # Check if this is a document row (has Type, Description, Document columns)
                link_cell = cells[2] if len(cells) > 2 else None
                if link_cell:
                    link = link_cell.find('a', href=True)
                    if link and link['href'].endswith('.htm') and not link['href'].endswith('-index.htm'):
                        # Found the primary document
                        doc_url = 'https://www.sec.gov' + link['href']
                        break
        else:
            # Fallback: Try to construct URL from accession number
            # Extract accession number from index URL
            # e.g., /Archives/edgar/data/1915380/000119312523305669/0001193125-23-305669-index.htm
            import re
            match = re.search(r'/(\d{10})-(\d{2})-(\d{6})-index\.htm', url)
            if match:
                accession = f"{match.group(1)}-{match.group(2)}-{match.group(3)}"
                # Try common primary document name
                doc_url = url.replace('-index.htm', '.htm')
            else:
                raise Exception("Could not find primary document in index page")
    else:
        doc_url = url

    # Fetch the actual document
    response = requests.get(doc_url, headers=HEADERS, timeout=30)
    response.raise_for_status()

    soup = BeautifulSoup(response.content, 'html.parser')

    # For inline XBRL viewer, try to get the raw document instead
    if '/ix?doc=' in doc_url:
        # Extract the document path from inline viewer URL
        import re
        match = re.search(r'/ix\?doc=(.+)', doc_url)
        if match:
            raw_doc_url = 'https://www.sec.gov' + match.group(1)
            try:
                response = requests.get(raw_doc_url, headers=HEADERS, timeout=30)
                response.raise_for_status()
                soup = BeautifulSoup(response.content, 'html.parser')
            except:
                pass  # Use the inline viewer version

    # Remove scripts, styles
    for tag in soup(['script', 'style', 'meta', 'link']):
        tag.decompose()

    text = soup.get_text(separator=' ', strip=True)
    text = ' '.join(text.split())  # Clean whitespace

    return text[:50000]  # Limit to first 50k chars


def check_for_extension(filing_date: datetime, content: str) -> dict:
    """Use AI to check if filing contains deadline extension"""

    excerpt = content[:30000]  # First 30k chars

    prompt = f"""
Analyze this SEC 8-K filing from {filing_date.strftime('%Y-%m-%d')} for deadline extensions and redemptions.

Look for:
1. Extension of combination period / deadline
2. New deadline date
3. Extension payment amount (deposit to trust)
4. Extension number (1st, 2nd, 3rd, etc.)
5. **Redemptions**: How many shares were redeemed?
6. **Redemption Price**: Price per share paid for redemptions
7. **Remaining Cash**: Trust account cash after redemptions

Return JSON:
{{
    "is_extension": true/false,
    "new_deadline": "YYYY-MM-DD" or null,
    "extension_payment": 100000 (numeric, no $ or commas) or null,
    "extension_number": 1 (integer) or null,
    "shares_redeemed": 1000000 (numeric shares redeemed) or null,
    "redemption_price": 10.00 (price per share) or null,
    "remaining_trust_cash": 50000000 (total cash in trust after redemptions) or null,
    "summary": "Brief description"
}}

If NOT an extension filing, return {{"is_extension": false, "summary": "Brief description of what this 8-K is about"}}

Filing text:
{excerpt}
"""

    try:
        response = AI_CLIENT.chat.completions.create(
            model="deepseek-chat",
            messages=[
                {"role": "system", "content": "You are an SEC filing analyst. Extract deadline extension data precisely. Return valid JSON."},
                {"role": "user", "content": prompt}
            ],
            response_format={"type": "json_object"},
            temperature=0
        )

        data = json.loads(response.choices[0].message.content)
        return data

    except Exception as e:
        print(f"      ‚ùå AI extraction failed: {e}")
        return {"is_extension": False, "summary": "Error analyzing"}


def main():
    print("üîç DMYY Deadline Extension Tracer")
    print("=" * 60)

    # Fetch all 8-Ks
    filings = fetch_recent_8ks()

    extensions = []
    other_filings = []

    for i, filing in enumerate(filings, 1):
        print(f"\n[{i}/{len(filings)}] {filing['date'].strftime('%Y-%m-%d')} - Analyzing...")

        # Fetch content
        try:
            content = fetch_filing_content(filing['url'])
        except Exception as e:
            print(f"   ‚ùå Failed to fetch: {e}")
            continue

        # Check for extension
        result = check_for_extension(filing['date'], content)

        if result.get('is_extension'):
            print(f"   ‚úÖ EXTENSION FOUND!")
            print(f"      New Deadline: {result.get('new_deadline')}")
            print(f"      Payment: ${result.get('extension_payment'):,}" if result.get('extension_payment') else "      Payment: N/A")
            print(f"      Extension #{result.get('extension_number')}" if result.get('extension_number') else "")

            # Redemption data
            if result.get('shares_redeemed'):
                print(f"      üîÑ Redemptions: {result.get('shares_redeemed'):,} shares @ ${result.get('redemption_price', 'N/A')}")
            if result.get('remaining_trust_cash'):
                print(f"      üí∞ Trust Cash Remaining: ${result.get('remaining_trust_cash'):,}")

            print(f"      Summary: {result.get('summary')}")

            extensions.append({
                'date': filing['date'].strftime('%Y-%m-%d'),
                'url': filing['url'],
                **result
            })
        else:
            print(f"   ‚ÑπÔ∏è  Not an extension: {result.get('summary', 'Unknown')}")
            other_filings.append({
                'date': filing['date'].strftime('%Y-%m-%d'),
                'summary': result.get('summary', 'Unknown')
            })

    # Print summary
    print("\n" + "=" * 60)
    print(f"üìä SUMMARY")
    print("=" * 60)
    print(f"Total Extensions Found: {len(extensions)}")

    if extensions:
        print("\nüóìÔ∏è  Extension Timeline:")
        for ext in extensions:
            print(f"   {ext['date']}: Deadline extended to {ext.get('new_deadline', 'N/A')}")
            if ext.get('extension_payment'):
                print(f"              Payment: ${ext['extension_payment']:,}")
            if ext.get('shares_redeemed'):
                print(f"              Redemptions: {ext['shares_redeemed']:,} shares @ ${ext.get('redemption_price', 'N/A')}")
            if ext.get('remaining_trust_cash'):
                print(f"              Trust Cash: ${ext['remaining_trust_cash']:,}")

        # Find most recent extension
        latest = extensions[0]  # Already sorted by date desc
        print(f"\n‚úÖ CURRENT DEADLINE: {latest.get('new_deadline')}")
        print(f"   (from {latest['date']} extension)")

        if latest.get('remaining_trust_cash'):
            print(f"\nüí∞ CURRENT TRUST CASH: ${latest.get('remaining_trust_cash'):,}")

        # Save to file
        with open('/tmp/dmyy_extensions.json', 'w') as f:
            json.dump({'extensions': extensions, 'other_filings': other_filings}, f, indent=2)
        print(f"\nüíæ Full results saved to /tmp/dmyy_extensions.json")
    else:
        print("\n‚ö†Ô∏è  No extensions found in analyzed filings")


if __name__ == "__main__":
    main()
